# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a full-stack RAG (Retrieval-Augmented Generation) healthcare assistant system with three main components:

1. **waig-rag-poc**: Core RAG backend with document processing and vector search
2. **rag-poc-middleware-app**: FastAPI middleware for chat orchestration with RabbitMQ worker
3. **rag-poc-user-app**: React/TypeScript frontend chat interface

## System Architecture

### Request Flow
```
User (React App)
  → Middleware API (FastAPI + MongoDB)
    → RabbitMQ Queue
      → Worker (background processing)
        → RAG API (waig-rag-poc)
          → Qdrant Vector Store + OpenAI
        ← Bot Response
  ← Chat Message
```

### Key Components

**waig-rag-poc (Port 8000 or 8888)**
- Handles document ingestion, chunking, and embeddings
- Qdrant vector store for hybrid search (vector + lexical)
- OpenAI GPT-4o-mini for generation with structured JSON responses
- LlamaParse for advanced PDF processing
- Hierarchical chunking for complex medical documents

**rag-poc-middleware-app (Port 8000)**
- User and chat message management via MongoEngine
- RabbitMQ-based async message queue
- Background worker auto-responds to human→bot messages
- JWT authentication with superuser tokens

**rag-poc-user-app (Port 3000)**
- Material-UI chat interface
- Real-time message polling
- Mobile-responsive with conversation/chatbox views

## Common Commands

### RAG Backend (waig-rag-poc)

```bash
cd waig-rag-poc

# Start RAG API server
python -m api.main
# or
uvicorn api.main:app --host 0.0.0.0 --port 8888 --reload

# Test with curl
curl -X POST "http://localhost:8888/query" \
  -H "Content-Type: application/json" \
  -d '{"query": "What is the treatment protocol?", "use_query_expansion": true}'

# Upload documents
curl -X POST "http://localhost:8888/documents/upload" \
  -F "files=@document.pdf"

# Upload JSON pages (hierarchical chunking)
curl -X POST "http://localhost:8888/documents/upload-json" \
  -H "Content-Type: application/json" \
  -d @parsed_jsons/document.json
```

### Middleware (rag-poc-middleware-app)

```bash
cd rag-poc-middleware-app

# Install dependencies
pip install -r requirements.txt

# Run setup script (creates superuser, bot, updates .env)
python setup_rag_system.py

# Start middleware server
python main.py
# or
uvicorn main:app --host 0.0.0.0 --port 8000 --reload

# Generate user token
curl -X POST "http://localhost:8000/api/v1/users/generate-token" \
  -H "Content-Type: application/json" \
  -d '{"name": "username", "password": "password"}'
```

### Frontend (rag-poc-user-app)

```bash
cd rag-poc-user-app

# Install dependencies
npm install

# Start development server
npm start

# Build for production
npm run build

# Run tests
npm test
```

## Environment Configuration

### waig-rag-poc/.env
Required keys:
- `OPENAI_API_KEY`: OpenAI API key
- `LLAMA_CLOUD_API_KEY`: LlamaParse API key
- `QDRANT_URL`: Default `http://localhost:6333`
- `OPENAI_MODEL`: Default `gpt-4o-mini-2024-07-18`
- `OPENAI_EMBEDDING_MODEL`: Default `text-embedding-3-large`

### rag-poc-middleware-app/.env
Required keys:
- `DATABASE_URL`: MongoDB connection (default: `mongodb://localhost:27017`)
- `DATABASE_NAME`: Database name (default: `fastapi_db`)
- `RABBITMQ_HOST`, `RABBITMQ_PORT`: RabbitMQ connection
- `RAG_API_URL`: RAG backend URL (default: `http://localhost:8888`)
- `SUPERUSER_TOKEN`: JWT token for worker (generated by setup script)
- `WORKER_ENABLED`: Enable background worker (default: `true`)
- `SECRET_KEY`: JWT signing key

### rag-poc-user-app/.env
- `REACT_APP_API_URL`: Middleware API URL

## Important Architecture Notes

### RAG Pipeline (waig-rag-poc/core/)

**rag_pipeline.py**: Main orchestration
- `AdvancedRAGPipeline.query()`: Executes retrieval with optional query expansion and reranking
- `process_documents()`: Processes PDFs via LlamaParse
- `process_json_pages()`: Uses hierarchical chunking for structured JSON documents
- Returns structured JSON matching `RAG_RESPONSE_SCHEMA` in config.py

**hierarchical_chunker.py**: Medical document chunking
- Preserves hierarchical structure (headings, sections, subsections)
- Context-aware chunking maintains parent-child relationships
- Better for complex clinical guidelines

**vector_store.py**: Qdrant integration
- Hybrid search combines vector (semantic) + lexical (BM25) search
- Default alpha: 0.7 vector, 0.3 lexical
- Metadata filtering by document name and page number

**embeddings.py**: OpenAI embeddings
- `OpenAIEmbeddings.embed_chunks()`: Batch embedding generation
- `QueryExpansionEmbeddings`: Generates alternative query phrasings for better recall
- `SemanticSimilarityRanker`: Post-retrieval reranking

**enhanced_lexical_search.py**: BM25 lexical search
- Medical keyword boosting
- Complements vector search for exact term matching

### Middleware Architecture (rag-poc-middleware-app/)

**Worker System** (app/services/worker.py):
- Continuously polls RabbitMQ queue
- On human→bot message: queries RAG API and creates bot response
- Uses `SUPERUSER_TOKEN` to authenticate bot responses
- Runs in background thread on server startup

**Chat Flow** (app/api/v1/endpoints/chat.py):
- POST to `/api/v1/chat/`: Creates message, publishes to RabbitMQ if human→bot
- GET `/api/v1/chat/messages/`: Retrieves conversation history with pagination
- Messages have offset field for ordering

**User System** (app/models/user.py):
- Two user types: `human` and `bot`
- Humans require email + hashed password
- Bots don't require authentication credentials

### Frontend Architecture (rag-poc-user-app/src/)

**ChatScreen.tsx**: Main chat interface
- Uses ResizableLayout for desktop split view
- Mobile: toggles between Conversations and ChatBox views
- Polls backend every 2 seconds for new messages

**api.ts**: Backend communication
- `sendMessage()`: Posts to middleware
- `getMessages()`: Fetches conversation history
- `login()`: Authenticates and stores JWT token

**narration.ts**: Text-to-speech integration (AWS Polly)
- Auto-narrates bot responses
- Used for accessibility features

## Development Workflow

### Starting the Full Stack

1. **Start Qdrant** (Docker):
   ```bash
   docker run -p 6333:6333 qdrant/qdrant
   ```

2. **Start MongoDB**:
   ```bash
   mongod
   ```

3. **Start RabbitMQ**:
   ```bash
   rabbitmq-server
   ```

4. **Start RAG Backend**:
   ```bash
   cd waig-rag-poc
   python -m api.main
   ```

5. **Start Middleware**:
   ```bash
   cd rag-poc-middleware-app
   python main.py
   ```

6. **Start Frontend**:
   ```bash
   cd rag-poc-user-app
   npm start
   ```

### Adding New Documents

**Via PDF Upload**:
```bash
curl -X POST "http://localhost:8888/documents/upload" \
  -F "files=@path/to/document.pdf"
```

**Via JSON (Hierarchical)**:
1. Place JSON in `waig-rag-poc/parsed_jsons/`
2. JSON format: `{"pages": [{"page": 1, "text": "...", "items": [...]}]}`
3. Upload via `/documents/upload-json` endpoint

### Testing the RAG System

1. Upload test documents to RAG backend
2. Verify indexing via `/documents` endpoint
3. Test queries via `/query` endpoint or through chat interface
4. Monitor worker logs in middleware for message processing

## Key Configuration Files

- **waig-rag-poc/config.py**: RAG settings, response schema, model configuration
- **rag-poc-middleware-app/app/core/config.py**: Middleware settings, RabbitMQ, database
- **waig-rag-poc/api/main.py**: RAG API endpoints and background processing
- **rag-poc-middleware-app/main.py**: Middleware startup with worker initialization

## Critical Implementation Details

1. **Hybrid Search**: Vector search alone may miss exact terms. The system uses weighted combination (alpha=0.7 vector, 0.3 lexical) for better recall on medical terminology.

2. **Hierarchical Chunking**: For JSON documents with structure (headings, sections), use `/documents/upload-json` instead of PDF upload to preserve document hierarchy.

3. **Query Expansion**: Enable with `use_query_expansion=true` to generate alternative query phrasings. Improves recall for complex queries.

4. **Background Processing**: Document uploads return a `task_id`. Poll `/documents/status/{task_id}` to check processing status.

5. **Worker Authentication**: The background worker uses `SUPERUSER_TOKEN` to create bot responses. If bot isn't responding, verify token is valid and user exists.

6. **Message Ordering**: Chat messages use `offset` field for ordering, not timestamps. When retrieving messages, sort by offset ascending.

7. **Confidence Scoring**: RAG responses include `confidence_score` (0-1). Scores below 0.3 indicate low confidence; UI should handle accordingly.

8. **Response Schema**: All RAG responses follow `RAG_RESPONSE_SCHEMA` with status, answer, confidence, sources, and metadata. Parse accordingly.

## Troubleshooting

**RAG API not initializing**:
- Check `OPENAI_API_KEY` and `LLAMA_CLOUD_API_KEY` in .env
- Verify Qdrant is running on port 6333
- Check logs for specific initialization errors

**Bot not responding**:
- Verify worker is enabled (`WORKER_ENABLED=true`)
- Check `SUPERUSER_TOKEN` is valid and matches a user
- Verify RabbitMQ is running and accessible
- Check `RAG_API_URL` points to correct RAG backend

**Frontend can't connect**:
- Verify middleware is running on expected port
- Check CORS settings in middleware config
- Verify JWT token is being sent with requests

**Document processing fails**:
- Check file is valid PDF or JSON format
- Verify LlamaParse API quota isn't exceeded
- Check Qdrant collection exists and is accessible
- Monitor background processing logs

**Low confidence scores**:
- Documents may not contain relevant information
- Try enabling query expansion and reranking
- Check if correct documents are uploaded
- Verify query is within document domain
